"""FccJob Module

  This module implements FccJob class which provides Fcc job definition.

  Here is an example of how to use FccJob to run FCC PHYSICS::

  from ILCDIRAC.Interfaces.API.NewInterface.FccJob import FccJob
  from ILCDIRAC.Interfaces.API.NewInterface.Applications.Fcc import FccAnalysis

  j = FccJob()

  FccPhysics = FccAnalysis(
    fcc_conf_file=
    '/cvmfs/fcc.cern.ch/sw/0.7/fcc-physics/0.1/x86_64-slc6-gcc49-opt/share/ee_ZH_Zmumu_Hbb.txt',

    fcc_output_file="ee_ZH_Zmumu_Hbb.root",
  )

  j.append(FccPhysics)
  jobID = j.submit()

"""

# standard libraries
import sys
import os

# It prints DIRAC environment script path available on AFS/CVMFS
# If you have installed the DIRAC client
# environment script must be here : ~/dirac/bashrc

DIRAC_ENV_MESSAGE = ["DIRAC environment :"]
DIRAC_ENV_MESSAGE += ["Please ensure that you set up correctly DIRAC environment e.g. :"]
DIRAC_ENV_MESSAGE += ['source /afs/cern.ch/eng/clic/software/DIRAC/bashrc']

def _init_dirac():
  """This function checks DIRAC environment."""

  # DIRAC environment
  try:
    os.environ["DIRAC"]
  except KeyError:
    # Print default path of environment script as 'help'
    # Here we use python quit() function and we do not use dirac exit
    # because DIRAC libraries are not yet imported
    print '\n'.join(DIRAC_ENV_MESSAGE)
    quit()

_init_dirac()

# After DIRAC environment checking done, we can import DIRAC libraries

# DIRAC libraries
from DIRAC.Core.Base import Script
Script.parseCommandLine()

from ILCDIRAC.Interfaces.API.DiracILC import DiracILC
from ILCDIRAC.Interfaces.API.NewInterface.UserJob import UserJob
from DIRAC import gLogger, exit as dexit


class FccJob(UserJob):
  """FccJob class is a definition of a FCC job.
  It inherits from the inheritance chain :
    -  UserJob -> Job

  In addition to UserJob functionalities,
  it proposes to split job over various parameters.

  Contrary to the parametric functions, bulk submission is done
  on client side and not on server side.
  """

  def __init__(self, script=None, repository_name=""):

    super(FccJob, self).__init__(script)

    # DiracILC instance creation
    # 'False' means no repository, else put 'name_of_your_repo'
    # 'name_of_your_repo' is a file generated by DIRAC which can be usefull
    # It contains infos like job_id etc...
    self.ILC = DiracILC(False) if not repository_name else DiracILC(True, repository_name)

    # By pass user prompt before submission
    # like that we submit directly
    # without waiting for user shell interaction
    self.ILC.checked = True

    self._data = set()
    self._are_data_consumed_by_splitting = False
    self.mode = ""
    self._switch = {}
    self.njobs = None
    self.events_per_job = None

    self._user_applications = set()

    # Used for the checking of many fccsw installations
    self._user_fccsw_applications = set()

    self._output_sandbox = set()
    self._input_sandbox = set()
    self._fcc_step = 0

  def append(self, application):
    """Redefinition of Dirac.Interfaces.API.Job.append()
    in order to save applications in a list.

    We do not append now, we make splitting stuff and 're-compute'
    each application before really appending applications in
    _add_application() method.

    """

    if application in self._user_applications:
      gLogger.error("You try to append many times the same application, please fix it !")
      dexit(1)

    #super(FccJob, self).append(application)
    self._user_applications.add(application)

    # Used for the checking of many fccsw installations
    if application.fcc_app_name.startswith('FccSw'):
      self._user_fccsw_applications.add(application)


  def setInputData(self, lfns):
    """Redefinition of Dirac.Interfaces.API.Job.setInputData()
    in order to save input data into a set.

    We do not set input data now, we just save all them for the moment.

    In the splitting stuff, new jobs may be created. If the
    'by_data' method is used then submit a job per input data.

    Do not send each job with all input data, we
    have to set the right data for each job, data that
    have been saved here in 'data' set.

    The user can specify input data to tell DIRAC to download input data
    (stored in DESY-SRM for example) on the CE.

    Data are registered to the DIRAC catalog and have a tape backend (OutputSE)
    Please refer to the documentation to see how to add files to the catalog.

    """

    self._data = self._output_sandbox.union(lfns) if isinstance(lfns, list) else self._data.union([lfns])
    #super(FccJob, self).setInputData(lfns)

  def setInputSandbox(self, files):
    """Redefinition of Dirac.Interfaces.API.Job.setInputSandbox()
    If the user want to use this function, it may 'erase' the input
    sandbox computed in FCC application.

    So we save job inputs here and add them
    later in _add_application() method to the FCC application inputs.
    """
    self._input_sandbox = self._input_sandbox.union(files) if isinstance(files, list) else self._input_sandbox.union([files])

  def setOutputSandbox(self, files):
    """Redefinition of Dirac.Interfaces.API.Job.setOutputSandbox()
    If the user want to use this function, it may 'erase' the output
    sandbox computed in FCC application.

    So we save job outputs here and add them
    later in _send_job() method to the FCC application outputs.
    """
    self._output_sandbox = self._output_sandbox.union(files) if isinstance(files, list) else self._output_sandbox.union([files])

  def submit(self, split=None, njobs=None, events_per_job=None, mode="wms"):
    """Redefinition of ILCDIRAC.Interfaces.API.NewInterface.UserJob.submit()
    to add splitting job stuff.

    There are 3 types of submission :

    - local without agent agent machinery
    - local with agent
    - grid

    The advantage of the Local submission mode is
    that jobs are immediately executed on the local resource.

    :param split: The splitting method to use
    :type split: str

    :param njobs: The number of jobs to use for the splitting
    :type njobs: str or int

    :param events_per_job: The number of events to use for the splitting
    :type events_per_job: str or int

    :param mode: The submission mode
    :type mode: str

    :return: the id(s)
    :rtype: list

    :Example:

    >>> job.submit(split="by_event", njobs="4", mode ='wms')

    """

    self.mode = mode

    self.events_per_job = self._to_int(events_per_job)
    self.njobs = self._to_int(njobs)

    if False is self.njobs or False is self.events_per_job:
      dexit(1)

    # Switch case python emulation
    self._switch = {"by_event" : self._split_by_events,
            "by_data" : self._split_by_data, None : self._atomic_submission}

    gLogger.info("DIRAC : DIRAC submission beginning...")

    if not self._checkFccJobConsistency(split):
      gLogger.info("DIRAC : DIRAC submission failed")
      dexit(1)

    job_ids = self._switch[split]()

    if not job_ids:
      gLogger.info("DIRAC : DIRAC submission failed")
      dexit(1)

    gLogger.info("DIRAC : DIRAC submission ending")

    return job_ids

  ###############################  FccJob FUNCTIONS ##############################################

  def _add_application(self, application):
    """This function adds an application to the job,
    catches error not raised by DIRAC and sets input sandbox.

    :param application: The application you want to add
    :type application: ILCDIRAC.Interfaces.API.NewInterface.Applications.FCC

    :Example:

    >>> self._add_application(FCC_PHYSICS)

    """

    application.fcc_app_index = '%s_%d' % (application.fcc_app_name, self._fcc_step)

    self._fcc_step += 1

    debug_message = "Application : "
    debug_message += "Application '%s' appending..." % application.fcc_app_name
    gLogger.debug(debug_message)

    # If user calls setInputSandbox, we get the files in
    # the set '_input_sandbox' and add them to
    # the temporary input sandbox of applications for a future checking.
    # The temporary input sandbox of each application is 'extended'
    # At the end, the final input sandbox take the set of
    # all these files.
    # Indeed, We merge input sandbox files given at the application level
    # with the input sandbox files given at the job level.
    if self._input_sandbox:
      application._temp_input_sandbox = application._temp_input_sandbox.union(self._input_sandbox)

    try:
      app_addition = super(FccJob, self).append(application)
    except Exception:
      exc_type, exc_value, exc_traceback = sys.exc_info()
      #print exc_type, exc_value, exc_traceback

      if 'NoneType' in str(exc_value):
        # The problem is that when the proxy is outdated, DIRAC got an exception
        # but this exception is not raised, a message is printed to the stdout
        # (saying that PEM files are outdated).
        error_message = "Application add operation : "
        error_message += "Please, configure your proxy before submitting a job from DIRAC"
        error_message += "\nIf you did not set up a proxy, try to refresh it "
        error_message += "by typing :\n"
        error_message += "dirac-proxy-init\n"
      else:
        error_message = "Application add operation : Error in the description of the module"
        error_message += "\nPlease pay attention to this message :"
        error_message += str(exc_type) + " : " + str(exc_value) + "\n"

      gLogger.error(error_message)
      return False

    if 'OK' in app_addition and not app_addition['OK']:
      error_message = "Application appending : "
      error_message += "Application '%s' appending failed" % application.fcc_app_name
      gLogger.error(error_message)
      gLogger.error(app_addition['Message'])
      return False

    debug_message = "Application '%s' appending successfull" % application.fcc_app_index
    gLogger.debug(debug_message)

    return True

  def _atomic_submission(self):
    """This function submits only one job, no splitting."""

    info_message = "Job splitting : No splitting to apply,then 'atomic submission' will be used"
    gLogger.info(info_message)

    if self.njobs:
      error_message = ["Atomic submission : You did not specify a splitting method"]
      error_message += ["So you do not have to set the number of jobs"]
      error_message += ["If you want to split your job over the number of events"]
      error_message += ["Please choose 'by_event' method"]
      gLogger.error('\n'.join(error_message))
      return False

    for application in self._user_applications:
      if not self._add_application(application):
        return False

    # send one job
    return self._send_job()

  def _checkFccJobConsistency(self, split):
    """This function :

    - Checks if FccJob parameters are valid.
    - Detects if the user try to work with many FCCSW installations.

    :param split: the splitting method to apply
    :type split: str

    :return: success or failure of the consistency checking
    :rtype: bool

    :Example:

    >>> self._checkFccJobConsistency(split)

    """

    gLogger.info("FccJob : FccJob _checkFccJobConsistency()...")

    if not self._user_applications:
      error_message = ["FccJob : Your job is empty !"]
      error_message += ["You have to append at least one application"]
      gLogger.error('\n'.join(error_message))
      gLogger.error("FccJob : FccJob _checkFccJobConsistency failed")
      return False

    if split not in self._switch:
      error_message = ["Job splitting : Bad split value"]
      error_message += ["Possible values are :"]
      error_message += ["- by_data"]
      error_message += ["- by_event"]
      error_message += ["- None"]
      gLogger.error('\n'.join(error_message))
      gLogger.error("FccJob : FccJob _checkFccJobConsistency() failed")
      return False

    if self._user_fccsw_applications:
      fccsw_path = next(iter(self._user_fccsw_applications)).fccsw_path

      if not all(app.fccsw_path == fccsw_path for app in self._user_fccsw_applications):
        error_message = "Submission : You can't have many FCCSW applications"
        error_message += " running with different installations of FCCSW"
        gLogger.error(error_message)
        gLogger.error("FccJob : FccJob _checkFccJobConsistency failed()")
        return False

    info_message = "FccJob : FccJob _checkFccJobConsistency() successfull"
    gLogger.info(info_message)

    return True

  def _print_submission(self, SUBMISSION):
    """This function interprets dictionnary result returned by DIRAC submit call and prints
    relevant informations like ID of the job etc...

    :param SUBMISSION: the returned value of submit call
    :type SUBMISSION: str

    :return: id of the job
    :rtype: str

    :Example:

    >>> self._print_submission(SUBMISSION)

    """

    job_id = None

    if 'OK' in SUBMISSION and not SUBMISSION['OK']:
      submission_message = ['Submission : Submission Failure']
      submission_message += ['Please check the description of your job']
      submission_message += ['Pay attention to the following message :']
      submission_message += [str(SUBMISSION['Message'])]
      gLogger.error('\n'.join(submission_message))
      return False

    # no job ID is given in local submission
    if 'JobID' in SUBMISSION:
      job_id = SUBMISSION['JobID']
      submission_message = ["GRID Submission : The job with the following ID : "]
      submission_message += [str(job_id)]
      submission_message += ["has been submitted to the grid"]
      submission_message += ["You can get output by visiting the DIRAC portal of your VO"]
      submission_message += ["in the 'Job Monitor' tab or by typing :"]
      submission_message += ["dirac-wms-job-get-output %d" % job_id]
    else:
      job_id = "NO_ID_IN_LOCAL_SUBMISSION"
      submission_message = ["Local Submission : The job has been submitted locally"]
      submission_message += ["Output results should be retrieved in the Local_* folder"]

    gLogger.info('\n'.join(submission_message))

    return job_id

  def _send_job(self):
    """This function submits a job which may have been 're-computed' by the splitting stuff
    and set output sandbox.
    """

    # We set the ouput sandbox here, after getting output sandbox files
    # in setOutputSandbox() method

    for application in self._user_applications:
      self._output_sandbox = self._output_sandbox.union(application._output_sandbox)
      #self._input_sandbox = self._input_sandbox.union(application._input_sandbox)

    if self._output_sandbox:
      result = super(FccJob, self).setOutputSandbox(list(self._output_sandbox))

      if 'OK' in result and not result['OK']:
        error_message = "Output Sandbox : Error in setting output sandbox"    
        gLogger.error(error_message)
        return False

    # Duplicates problem for setInputSandbox :
    # Calling many times setInputSandbox keep the last list and add new list
    # to the old one in the JDL (we use instead the list attribute 'inputSB' of Application)

    #if self._input_sandbox:
    #    super(FccJob, self).setInputSandbox(list(self._input_sandbox))

    #print super(FccJob, self)._toJDL()

    # Because we redefined setInputData() method
    # do not forget to set them for the job except
    # when _split_by_data() method is used (hence '_are_data_consumed_by_splitting' attribute)

    if self._data and not self._are_data_consumed_by_splitting:
      result = super(FccJob, self).setInputData(list(self._data))
      
      if 'OK' in result and not result['OK']:
        error_message = "Input Data : Error in setting input data"    
        gLogger.error(error_message)
        return False

    submission = super(FccJob, self).submit(self.ILC, mode=self.mode)

    # Reset app counter
    self._fcc_step = 0

    return self._print_submission(submission)

  def _split_by_data(self):
    """This function submits a job per input data."""

    info_message = "Job splitting : Splitting 'by_data' method..."
    gLogger.info(info_message)

    # Ensure that data have been specified by setInputData() method
    if not self._data:
      error_message = ["Job splitting : Can not continue, missing input data"]
      error_message += ["Job splitting process needs input data"]
      gLogger.error('\n'.join(error_message))
      return False

    self._are_data_consumed_by_splitting = True

    job_ids = []

    # Here, we submit a job for each data
    for data in self._data:
      # Job level
      # Like that the data will be downloaded in the job PWD
      result = super(FccJob, self).setInputData(data)

      if 'OK' in result and not result['OK']:
        error_message = "Input Data : Error in setting input data"    
        gLogger.error(error_message)
        return False

      # application level
      for application in self._user_applications:
        # For FCCSW :
        # This file is given as input file for FCCDataSvc()
        # in the gaudi configuration file 'gaudi_options.py'
        application._fcc_input_data = [data]

        if not self._add_application(application):
          return False

      # Send a job per input data
      job_id = self._send_job()

      if not job_id:
        return False

      job_ids.append(job_id)

    return job_ids

  def _split_by_events(self):
    """This function submits a job per subset of events."""

    info_message = "Job splitting : splitting 'by_events' method..."
    gLogger.info(info_message)

    # All applications must have the same number of events
    # We can get this number from the first application for example
    total_number_of_events = next(iter(self._user_applications)).NumberOfEvents

    # Ensure that all applications have the same total number of events
    if not all(app.NumberOfEvents == total_number_of_events for app in self._user_applications):
      error_message = ["Job splitting : In splitting 'by_events' method :"]
      error_message += ["Applications must have the same number of events"]
      gLogger.error('\n'.join(error_message))
      return False

    # 1st case : submit(njobs=3,events_per_job=10)
    # trivial case => each job (total of 3) run applications of 10 events each
    # Do not consider number of event of the application (overwrite it)
    # it is done after

    if self.events_per_job and self.njobs:
      debug_message = "Job splitting : 1st case"
      debug_message += " events per job and number of jobs has been given (easy)"
      gLogger.debug(debug_message)

      map_event_job = [self.events_per_job] * self.njobs

    # 2nd case : submit(split="by_event",events_per_job=10)
    # In this case the number of events has to be set inside applications
    # otherwise outputs error.
    # Given the number of events per job and total of number of event we want,
    # we can compute the unknown which is the number of jobs

    elif self.events_per_job and total_number_of_events:
      debug_message = "Job splitting : 2nd case"
      debug_message += " only events per job has been given but we know the total"
      debug_message += " number of events, so we have to compute the number of jobs required"
      gLogger.debug(debug_message)

      if self.events_per_job > total_number_of_events:
        error_message = "Job splitting : The number of events per job has to be"
        error_message += " lower than or equal to the total number of events"
        gLogger.error(error_message)
        return False


      number_of_jobs_int_div = total_number_of_events / self.events_per_job
      number_of_jobs_rest = total_number_of_events % self.events_per_job

      map_event_job = [self.events_per_job] * number_of_jobs_int_div

      map_event_job += [number_of_jobs_rest] if number_of_jobs_rest != 0 else []

    # 3rd case : submit(split='by_event', njobs=10)
    # So the total number of events has to be set inside application
    # If not then output error

    else:
      debug_message = "Job splitting : 3rd case"
      debug_message += " The number of jobs has to be given and the total number"
      debug_message += "  of events has to be set"
      gLogger.debug(debug_message)

      if (not total_number_of_events) or (total_number_of_events < self.njobs):
        error_message = ["Job splitting : The number of events has to be set"]
        error_message += ["It has to be greater than or equal to the number of jobs"]
        gLogger.error('\n'.join(error_message))
        return False

      event_per_job_int_div = total_number_of_events / self.njobs
      event_per_job_rest = total_number_of_events % self.njobs

      map_event_job = [event_per_job_int_div] * self.njobs

      if event_per_job_rest != 0:
        for suplement in range(event_per_job_rest):
          map_event_job[suplement] += 1


    debug_message = ["Job splitting : Here is the 'distribution' of events over the jobs"]
    debug_message += ["A list element corresponds to a job and the element value"]
    debug_message += ["is the related number of events"]
    debug_message += [str(map_event_job)]
    gLogger.debug('\n'.join(debug_message))

    job_ids = []

    for event_per_job in map_event_job:
      for application in self._user_applications:
        application.NumberOfEvents = event_per_job
        if not self._add_application(application):
          return False

      # Send a job according the mapping : map_event_job
      job_id = self._send_job()

      if not job_id:
        return False

      job_ids.append(job_id)

    return job_ids

  def _to_int(self, number):
    """This function casts number parameter to an integer.
    It also accepts 'string integer' parameter.

    :param number: the number to cast (number of event, number of job)
    :type number: str or int

    :return: success or failure of the casting
    :rtype: bool

    :Example:

    >>> self._to_int("1000")
    """

    if not number:
      return number

    try:
      number = int(number)
      if number <= 0:
        raise ValueError
    except ValueError:
      error_message = ["Job splitting : Please, enter valid numbers :"]
      error_message += ["'events per job' and 'number of jobs' must be positive integers"]
      gLogger.error('\n'.join(error_message))
      return False

    return number
